1. 概要：
本プロジェクトでは、大量のテキスト・ドキュメント情報を詳細度 (粒度；Granularity) 別に構造化し、異なる２/３次元空間にて視覚的に表現するソフトウェア開発を行います。図１では、多層２次元空間に構造化された情報のイメージを示しています。従来の線形的なテキストでは、縦あるいは横へと一方方向に流れてしまい、情報の構造化が困難です。仮に章や節ごとに階層化されていても、情報間の関連性や全体像を“直感的に”把握することは容易ではありません。一方で、テキストのベクトル化とクラスタリングによって情報の構造化が可能となったものの、情報の概要と詳細、あるいは抽象的・具体的な情報が同じ空間内にマッピングされているため、多くの情報が与えられすぎてしまいます。これによって、情報のオーバーロードや処理キャパシティを超えてしまうことが課題です。
提案するソフトウェアでは、これらの課題を解決するために、ベクトル化されたテキストデータを情報の粒度に応じて異なる空間に階層的に配置します。これにより、ユーザーは俯瞰的な視点から徐々に詳細な情報へと相互的にナビゲートでき、知識の効率的な探索と理解を可能とする新しい認知科学的なアプローチを提供します。さらに、AIによる情報の要約・分析により、膨大な情報から必要な領域の特定と知識の抽出を迅速に行うことが可能なプラットフォームを目指します。

図１：詳細度別に多層２次元空間に階層化されたテキスト情報の例。トップ層は大まかなカテゴリーに基づくドキュメントクラスターを形成し、ボトム層はオリジナルの文章情報を保存。検索実行時は、トップ層からの段階的なフィルタリングがユーザーに俯瞰的な情報を提供し、彼らが探し求める詳細情報までのナビゲーションをサポートする。
2. 着想・提案に至るまでの経緯：
テキスト読解の非効率性：
私は長文テキストを読む際、情報を関連度別にカテゴライズするようなイメージで処理を行なっています。また、特定のキーワードや五感情報 (特に視覚) をトリガーとして関連する情報を引き出すため、一番最初に検索が実行される層には、詳細な情報を配置しません。なぜなら、抽象的な概念 (カテゴリー) と具体的な情報 (詳細な文章) が同じ検索層に混在することで、一度に捌ききれない量の情報が提供されるからです。つまり、単に線形的なテキストを読んで理解するという行為は非効率的ではないかという疑問が湧きました。

就職活動での実体験：
私が実践している処理と情報の取り出し方に関して、具体的な体験を紹介します。私は、企業の方との面接時に準備をせずに臨みました。この準備というのは、想定される質問を列挙して、回答の反復練習を行うという類いものです。つまり、私は面接官からの質問に対して、リアルタイムで自分の体験や記憶が保管されたデータベースから検索と情報取得を行う必要がありました。例えば、面接官が“高校時代”と発した瞬間に、私自身の中で「高校→部活→学んだこと->どうビジネスに活かす」などのように、情報取得と構造化が行われます。したがって、このような経験から情報の詳細度に応じた階層的な取得の重要性に気がつきました。

AIチャットボットとの対話：
多くの方がAIチャットを使用していますが、中には「どのような質問を投げかければ良いかわからない」という課題があると認識しました。また、私自身も目的の情報にアクセスするには、複数回クエリを実行して質問のアプローチの仕方を随時アップデートする必要があります。つまり、私たちが望む回答をAIに出力させるためには、それに適した解像度の高い質問が必要です。さらに、このような解像度の高い質問を考えるには、事前の知識や情報の俯瞰的な理解が必要であるという仮説が導き出されました。

これらの経験から、「なぜ人間は未だにテキストを記憶しづらい形で読んでいるのか？」「なぜ産業革命やインターネット革命を経ても、“読む”という行為に大きな変化が起きないのか？」という疑問が生まれました。特にAIの台頭により情報量が爆発的に増加する現代において、従来の線形的なテキスト処理では限界があります。このような背景から、情報を粒度別に異なる層に配置し、俯瞰的視点から詳細情報へと直感的にナビゲートできるソフトウェアの提案を行います。
3. 提案するソフトウェアの紹介：
開発予定のソフトウェアでは、以下の主要機能を実装します：
3.1. データ取込・処理エンジン：
多様な形式のドキュメントを取り込み、Embeddingモデルによってテキストデータを高次元ベクトル空間にマッピング
GoogleDriveとOneDriveなどの連携により、外部データベースからのドキュメントの取得を可能とする
LLMによる抽象的なドキュメントサマリーを生成し、これを情報の“概要・抽象レイヤー”として、オリジナルの文章を“詳細・具体レイヤー”として扱う
3.2. 階層構造構築モジュール：
情報の粒度別に異なる高次元空間の生成【ドキュメントクラスタ=> ドキュメントサマリ=> 文章クラスタ=> 文章】
空間を下に移動するにつれて、抽象的な概念から具体的な内容へと移行し、段階的に情報の範囲を絞り込みながら詳細度を高める構造を目指す
次元削減アルゴリズムとクラスタリング手法の組み合わせによって、高次元データを２/３次元空間への圧縮により視覚化を行う
コサイン類似度などの指標に基づき、異なる高次元空間同士 (ドキュメント<=>文章) の関連性分析と接続強度の計算を実施
3.3. インタラクティブ可視化インターフェース：
２/３次元空間における階層的データの動的可視化と、直感的な操作によるクラスター間の移動と探索
俯瞰的ビューからの段階的ズームインや、異なる情報粒度空間のシームレスなアクセスを実現
カーソルのホバーリングによる関連キーワードの表示と、オリジナルのテキストデータへの即時アクセスを実現
アップデートされた文章のスクロールに合わせて、現在のベクトル空間における位置情報を提示。“3.2. 他サービス・アプローチとの差別化”にて詳細を提示
3.4. 検索・クエリシステム：
自然言語クエリからの効率的なベクトル検索とドキュメント参照を提供 (NotebookLMと同等の機能)
検索結果の階層的表示によって、より俯瞰的な視点からの関係性の把握を推進
段階的な情報の絞り込み行い、ユーザーが求める情報へのアクセスための言語化とより適切な情報抽出のためのクエリをサポート

以下の機能は、プロジェクトの進み具合によりその都度スケーリングを行うつもりです。私自身もどれほどのスピードで開発を進めることができるのかが未知数なため、上記の主要機能を最優先として、以下の追加機能の実装に取り掛かるつもりです。
3.5. Embeddingモデル・LLMのファインチューニング：
ドキュメントクラスタ内のデータの事前処理を行い、EmbbdingモデルやLLMのファインチューニングの実施。Knowledge Distillationや強化学習を利用し、軽量なモデルにてパラメーターのアップデートを行う。
開発者向けに提供するAPIにて、アップロードされたドキュメントベースの独自モデルへアクセス可能とする。モデルパラメータの共有・公開も視野 (HuggingFaceとの連携など) 。
3.6. 広告最適化と戦略的配置：
本ソフトウェアでは、関連する情報やドキュメント毎にクラスタリングが実施されるため、各ユーザーの興味や関心の取得がリアルタイムで可能。
広告の掲示時間は約5秒ほどに設定し、ベクトル検索とLLMの出力時間を待つ間に投入することを予定。Freemiumプラン加入者のみに限る。
3.7. 新しいアプローチの探索・論文執筆：
一般的な次元削減技術、特にPCAなどでもある程度の結果 (２/３次元内でのセマンティックな情報構造) が得られると予測されますが、これらは事前にベクトル化されたドキュメント情報のみに基づく傾向。
RAGパイプラインでは、情報抽出・検索時にユーザークエリもベクトル化されるため、そのクエリベクトルも考慮した次元削減手法の導入を検討。いわば、ユーザーの質問に基づいて、非線形的に多次元空間を切り取り、２/３次元へ圧縮する技術の開発を行う。
3.8. キャラクターデザイン：
階層的情報構造と知識探索などの観点から”世界樹=>ユグドラシル) ”という象徴が形成されるので、数々の神話に登場する神様からキャラクターをデザイン、VR/ARの世界を見据えて背景デザインにも取り組む。
知識探索に関連する機能として、複数AIエージェントによるディスカッション機能の実装も検討。各エージェントが神様を象徴とする魅力的なキャラクターであればエンターテイメント性の向上や、ソフトウェアとしての機能を超えたコンテンツ化にも期待。
4. プロジェクト終了後のアクション：
AISaaSやAaaS (Agent as a Service) としてローンチし、保守・運用・アップデート
教育、学術などの特定分野、あるいは企業向けに特化したバージョンの開発
スマートフォンからのアクセスを可能とするために、モバイル版アプリケーションの開発
階層的情報構造化、次元削減、モデルトレーニングに関する論文を執筆し、学術誌やジャーナルへ投稿
企業からのBuyoutオファーがあった場合、私自身もチームに加わることを条件に売却も検討。グローバルで活躍する日本企業ならなお良いが。
5. 差別化・導入例：
5.1. 他サービス・アプローチとの差別化
まず既存のRAG (検索拡張) アプローチやベクトル検索との決定的な違いは、テキスト情報をその詳細度別に異なる高次元ベクトル空間に格納する点です。従来の方法では、すべてのテキスト情報が同じベクトルデータベースに保存されるため、検索結果には具体的で詳細な情報と、抽象的な情報の概要が混在しています。Facebookリサーチチームが開発した”FAISS”のInverted File Index (IVF) 手法では、ベクトルの量子化 (複数ベクトルの平均値の取得など) により階層的なツリー構造による情報検索が可能となりました。しかし、この階層化は、セマンティックな関連性のみに焦点が当てられ、情報の詳細度は考慮されていません。一方で、Hierarchical Navigation Small World (HNSW) 手法では、ベクトル量子化やクラスタリングが実装されず、ランダムに最上位の層 (Top Layer) が選択されます。ベクトル化されたテキスト情報との接続や関連性をナビゲートするような検索の手法を提供している点は参考となりますが、やはり”概要から詳細”というフローは未だ確立できていません。

次に、RAGを使った情報検索を提供するサービスの一つに、Google社が提供している“NotebookLM”があげられます。ここでは、NotebookLMの機能を比較し、本プロジェクトとの違いをまとめます：
NotebookLMでは、ドキュメント数に比例して、検索時間が長くなる傾向あり。つまり、情報の事前処理 (クラスタリング等) なしに、すべてのベクトル情報を検索していると予測。本ソフトウェアの検索手法は、ツリー構造での検索が行わるため、処理速度をある程度一定に保つことが可能。例えば、ドキュメントサマリーとの関連性が著しく低いと判定されれば、そのドキュメントに属する文章との関連度検索を省略。
NotebookLMでは、手動でドキュメントの絞り込みを行うため、不必要な情報が検索に利用される。また、ユーザー自身もどのドキュメントにどんな情報があるのかを直感的に理解することは不可能。本ソフトウェアでは、段階的なドキュメント絞り込みと俯瞰的な情報検索により、ユーザーが持つ真の要求やゴールに寄り添うことが可能。
本ソフトウェアは、情報の検索だけでなく構造化に着目している。つまり、検索を行うためには要求が言語化される必要がある。構造化によりその要求自体を生み出すこともサポート。

最後に、上記は情報の“検索”のみに着目していましたが、本プロジェクトでは“情報の理解”や“情報の構造化”にも焦点を当てています。例えば、ユーザーがアップロードされた文章を読み進める際に、構造化されたダイアグラムが「特定の文章はどのカテゴリに属し、さらにどのグループに位置しているのか」をリアルタイムで表示する機能を提供する予定です 。さらに、ユーザーのクエリによって情報のフィルタリングが行われた後には、各ドキュメントや文章同士の関連性を視覚的に認識できるようになります。


5.2. 予測される導入例
学術的な研究者：様々な文献から関連文章や引用のピックアップ、各文献や研究内容のクラスタリングも可能として、構造化をサポート。
学生・学習者：情報の整理と構造化、情報の検索。教材や各自のノートを含む 。
ハードウェア：PC上に保存されたファイルを自動で構造化、ファイリングを実施。
ビジネス：社内ドキュメントの自動ファイリングと可視化、Google DriveやOneDriveとの同期
金融マーケット関係者：リアルタイムで経済ニュースを分析し、構造化と視覚化を実施。特定の株価や指標に紐づけることで、データ主導のアクションをサポート。
5.3. グローバル化について
これまで言及してきた開発背景やソフトウェア機能にフォーカスした場合、グローバルでも通用するのではないかと感じています。というのも、本ソフトウェアが解決する課題や問題が、多く人間が潜在的に欲しているものである可能性があるからです。もしかしたら、ほとんどの人は情報の構造化・理解・知識取得に関して、生まれ持った天性的な何か (IQなど) に左右されると割り切っている可能性があります。現時点においては、サービスユーザー層やターゲット層が明確なペインを自覚していない可能性はあるものの、洗練されたユーザーインタフェースを提示することで「こんなものを探し求めていた」を引き出すことができると信じます。
次に、グローバルで展開する際に、言語に関する課題があげられますが、この点には問題ないと見ています。むしろ、多くのLLMやEmbeddingモデルは英語ベースで公開されています。さらに、この4年間は英語圏で多様なソフトウェアを使っていたので、私自身としても英語ベースから先に開発したいくらいです。ユーザーインターフェイスもシンプルさと機動性を重視する予定なので、独自的な機能以外においてはテンプレート等を活用する予定です。
6. 使用言語：
一つの言語やサービスにこだわらず、なるべく多くのものに触れたいと考えています。
(1) Next.js:
サーバサイドやクライアントサイド処理の違いに注意しながら、全フロントエンド処理を担当
特に`react-force-graph-3d`が今回のユーザーインターファイスでの視覚化のキーライブラリ
認証関連などの処理において、すでに確立されているテンプレートの方が良いと判断される場合には、バックエンドにも使用を検討
(2) Go: 
メインのバックエンドの処理に使用予定。DB連携、API処理、RESTfulAPI設計など。
特に同時処理 (Concurrency) が必要な場合や、gRPC導入の際に積極的にGoを優先したい
(3) Python: 
機械学習ライブラリに使用。今回は、ベクトル化されたテキストの次元削減とクラスタリングが中心。
LLMファインチューニングの際には必須なので、Goと連携できるPythonのAPI処理を確保 (gRPCを検討)
(4) クラウドサービス：
メインはAWSかGCP、サブでAzureを予定し、Terraformで複数クラウドを管理。Go中心ならGCPが良いかもだが、AWSもお堅い。
Azureはドキュメントの処理などのピンポイントで導入する場合あり
